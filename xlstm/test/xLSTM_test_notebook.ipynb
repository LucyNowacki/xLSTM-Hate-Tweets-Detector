{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {},
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "8902\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "print(torch.backends.cudnn.version())\n",
    "\n",
    "\n",
    "from xlstm.blocks.slstm.cell import sLSTMCell, sLSTMCellConfig\n",
    "from xlstm.blocks.slstm.block import sLSTMBlock, sLSTMBlockConfig\n",
    "from xlstm.blocks.slstm.layer import sLSTMLayer, sLSTMLayerConfig\n",
    "from xlstm.components.feedforward import FeedForwardConfig\n",
    "\n",
    "backend = \"cuda\" if torch.cuda.is_available() else \"vanilla\"\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Clean the torch extensions cache\n",
    "extension_cache = \"/home/lucy/.cache/torch_extensions/\"\n",
    "if os.path.exists(extension_cache):\n",
    "    shutil.rmtree(extension_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {},
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "cell = sLSTMCell(sLSTMCellConfig(hidden_size=128, num_heads=2, backend=\"vanilla\", dtype=\"bfloat16\", function=\"slstm\", enable_automatic_mixed_precision=False)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {},
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 4, 64]) torch.Size([4, 4, 128])\n"
     ]
    }
   ],
   "source": [
    "res = cell(torch.zeros([4, 4, 4*128], dtype=torch.bfloat16).to(device))\n",
    "print(res[0].shape, res[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "block = sLSTMBlock(sLSTMBlockConfig(slstm=sLSTMLayerConfig(embedding_dim=32,\n",
    "    num_heads=4, backend=\"vanilla\", function=\"lstm\", bias_init=\"standard\", recurrent_weight_init=\"standard\",\n",
    "    _block_idx=2, _num_blocks=4, enable_automatic_mixed_precision=True), feedforward=FeedForwardConfig())).to(device).to(dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "inp = torch.ones((3, 4, 32), dtype=torch.bfloat16).to(device)\n",
    "inp.requires_grad = True\n",
    "res = block(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 32])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasGemmStridedBatchedEx(handle, opa, opb, (int)m, (int)n, (int)k, (void*)&falpha, a, CUDA_R_16BF, (int)lda, stridea, b, CUDA_R_16BF, (int)ldb, strideb, (void*)&fbeta, c, CUDA_R_16BF, (int)ldc, stridec, (int)num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mres\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# check for causality, batch interconnect\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(inp\u001b[38;5;241m.\u001b[39mgrad[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msum(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m== 0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/lucy/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lucy/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasGemmStridedBatchedEx(handle, opa, opb, (int)m, (int)n, (int)k, (void*)&falpha, a, CUDA_R_16BF, (int)lda, stridea, b, CUDA_R_16BF, (int)ldb, strideb, (void*)&fbeta, c, CUDA_R_16BF, (int)ldc, stridec, (int)num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`"
     ]
    }
   ],
   "source": [
    "res[1, 1].sum().backward()\n",
    "# check for causality, batch interconnect\n",
    "print(inp.grad[2, 1].sum(), \"== 0\")\n",
    "print(inp.grad[0, 1].sum(), \"== 0\")\n",
    "print(inp.grad[1, 2].sum(), \"== 0\")\n",
    "print(inp.grad[1, 0].sum(), \"!= 0\")\n",
    "print(inp.grad[1, 1].sum(), \"!= 0\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- autocast: Automatically casts operations to bfloat16 where appropriate, falling back to float32 when necessary.\n",
    "- GradScaler: Scales gradients to prevent underflow during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 32])\n",
      "tensor(0., device='cuda:0', dtype=torch.bfloat16) == 0\n",
      "tensor(0., device='cuda:0', dtype=torch.bfloat16) == 0\n",
      "tensor(0., device='cuda:0', dtype=torch.bfloat16) == 0\n",
      "tensor(nan, device='cuda:0', dtype=torch.bfloat16) != 0\n",
      "tensor(nan, device='cuda:0', dtype=torch.bfloat16) != 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import shutil\n",
    "\n",
    "# Clean the torch extensions cache\n",
    "extension_cache = \"/home/lucy/.cache/torch_extensions/\"\n",
    "if os.path.exists(extension_cache):\n",
    "    shutil.rmtree(extension_cache)\n",
    "\n",
    "# Example of rebuilding or running your project\n",
    "# Ensure the model is on the correct device and using compatible data types\n",
    "block = sLSTMBlock(sLSTMBlockConfig(\n",
    "    slstm=sLSTMLayerConfig(\n",
    "        embedding_dim=32,\n",
    "        num_heads=4,\n",
    "        backend=\"vanilla\",  # Use the correct backend\n",
    "        function=\"lstm\",\n",
    "        bias_init=\"standard\",\n",
    "        recurrent_weight_init=\"standard\",\n",
    "        _block_idx=2,\n",
    "        _num_blocks=4,\n",
    "        enable_automatic_mixed_precision=True  # Enable AMP\n",
    "    ),\n",
    "    feedforward=FeedForwardConfig()\n",
    ")).to(device).to(dtype=torch.bfloat16)\n",
    "\n",
    "# Ensure input tensor is on the correct device and using compatible data type\n",
    "inp = torch.ones((3, 4, 32), dtype=torch.bfloat16).to(device)\n",
    "inp.requires_grad = True\n",
    "\n",
    "# Use mixed precision\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Run the forward pass\n",
    "with torch.cuda.amp.autocast():\n",
    "    res = block(inp)\n",
    "    print(res.shape)  # Verify the output shape\n",
    "\n",
    "# Run the backward pass\n",
    "scaler.scale(res[1, 1].sum()).backward()\n",
    "\n",
    "# Check gradients\n",
    "print(inp.grad[2, 1].sum(), \"== 0\")\n",
    "print(inp.grad[0, 1].sum(), \"== 0\")\n",
    "print(inp.grad[1, 2].sum(), \"== 0\")\n",
    "print(inp.grad[1, 0].sum(), \"!= 0\")\n",
    "print(inp.grad[1, 1].sum(), \"!= 0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from xlstm.xlstm_block_stack import xLSTMBlockStackConfig, xLSTMBlockStack, mLSTMBlockConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result shape: torch.Size([3, 4, 32])\n"
     ]
    }
   ],
   "source": [
    "block = sLSTMBlock(sLSTMBlockConfig(\n",
    "    slstm=sLSTMLayerConfig(\n",
    "        embedding_dim=32,\n",
    "        num_heads=4,\n",
    "        backend=\"vanilla\",\n",
    "        function=\"lstm\",\n",
    "        bias_init=\"standard\",\n",
    "        recurrent_weight_init=\"standard\",\n",
    "        _block_idx=2,\n",
    "        _num_blocks=4,\n",
    "        enable_automatic_mixed_precision=False\n",
    "    ),\n",
    "    feedforward=FeedForwardConfig()\n",
    ")).to(device).to(dtype=torch.bfloat16)\n",
    "\n",
    "inp = torch.ones((3, 4, 32), dtype=torch.bfloat16).to(device)\n",
    "inp.requires_grad = True\n",
    "res = block(inp)\n",
    "\n",
    "print(f\"Result shape: {res.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "bs = xLSTMBlockStack(\n",
    "    xLSTMBlockStackConfig(\n",
    "        slstm_block=sLSTMBlockConfig(slstm=sLSTMLayerConfig(backend=\"vanilla\")),\n",
    "        slstm_at=\"all\",\n",
    "        num_blocks=48,\n",
    "        embedding_dim=1024,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xLSTMBlockStack(\n",
       "  (blocks): ModuleList(\n",
       "    (0-47): 48 x sLSTMBlock(\n",
       "      (xlstm_norm): LayerNorm()\n",
       "      (xlstm): sLSTMLayer(\n",
       "        (conv1d): CausalConv1d(\n",
       "          (conv): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)\n",
       "        )\n",
       "        (conv_act_fn): SiLU()\n",
       "        (fgate): LinearHeadwiseExpand(in_features=1024, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "        (igate): LinearHeadwiseExpand(in_features=1024, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "        (zgate): LinearHeadwiseExpand(in_features=1024, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "        (ogate): LinearHeadwiseExpand(in_features=1024, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "        (slstm_cell): sLSTMCell_vanilla(function=slstm, hidden_size=1024, num_heads=4)\n",
       "        (group_norm): MultiHeadLayerNorm()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn_norm): LayerNorm()\n",
       "      (ffn): GatedFeedForward(\n",
       "        (proj_up): Linear(in_features=1024, out_features=2688, bias=False)\n",
       "        (proj_down): Linear(in_features=1344, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (post_blocks_norm): LayerNorm()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "bs = xLSTMBlockStack(xLSTMBlockStackConfig(mlstm_block=mLSTMBlockConfig(), context_length=2048, num_blocks=48, embedding_dim=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xLSTMBlockStack(\n",
       "  (blocks): ModuleList(\n",
       "    (0-47): 48 x mLSTMBlock(\n",
       "      (xlstm_norm): LayerNorm()\n",
       "      (xlstm): mLSTMLayer(\n",
       "        (proj_up): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "        (q_proj): LinearHeadwiseExpand(in_features=2048, num_heads=512, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "        (k_proj): LinearHeadwiseExpand(in_features=2048, num_heads=512, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "        (v_proj): LinearHeadwiseExpand(in_features=2048, num_heads=512, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "        (conv1d): CausalConv1d(\n",
       "          (conv): Conv1d(2048, 2048, kernel_size=(4,), stride=(1,), padding=(3,), groups=2048)\n",
       "        )\n",
       "        (conv_act_fn): SiLU()\n",
       "        (mlstm_cell): mLSTMCell(\n",
       "          (igate): Linear(in_features=6144, out_features=4, bias=True)\n",
       "          (fgate): Linear(in_features=6144, out_features=4, bias=True)\n",
       "          (outnorm): MultiHeadLayerNorm()\n",
       "        )\n",
       "        (ogate_act_fn): SiLU()\n",
       "        (proj_down): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (post_blocks_norm): LayerNorm()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.2.0\n",
      "Is CUDA available: True\n",
      "CUDA version: 12.1\n",
      "cuDNN version: 8902\n",
      "Compilation failed: Command '['/usr/local/cuda-12.1/bin/nvcc', '--generate-dependencies-with-compile', '--dependency-output', 'lstm_pointwise.cuda.o.d', '-ccbin', '/home/lucy/anaconda3/envs/deep/bin/x86_64-conda-linux-gnu-cc', '-DTORCH_EXTENSION_NAME=lstm_HS32']' returned non-zero exit status 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nvcc fatal   : No input files specified; use option --help for more information\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['PATH'] = '/usr/local/cuda-12.1/bin:' + os.environ['PATH']\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-12.1/lib64:' + os.environ.get('LD_LIBRARY_PATH', '')\n",
    "\n",
    "# Clean previous builds\n",
    "extension_cache = \"/home/lucy/.cache/torch_extensions/\"\n",
    "if os.path.exists(extension_cache):\n",
    "    subprocess.run(['rm', '-rf', extension_cache])\n",
    "\n",
    "# Verify PyTorch configuration\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "else:\n",
    "    print(\"CUDA not available\")\n",
    "\n",
    "# Rebuild the extension\n",
    "try:\n",
    "    subprocess.run([\n",
    "        '/usr/local/cuda-12.1/bin/nvcc',\n",
    "        '--generate-dependencies-with-compile',\n",
    "        '--dependency-output', 'lstm_pointwise.cuda.o.d',\n",
    "        '-ccbin', '/home/lucy/anaconda3/envs/deep/bin/x86_64-conda-linux-gnu-cc',\n",
    "        '-DTORCH_EXTENSION_NAME=lstm_HS32',\n",
    "        # Add other necessary flags and arguments\n",
    "    ], check=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Compilation failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.2.0\n",
      "Is CUDA available: True\n",
      "CUDA version: 12.1\n",
      "cuDNN version: 8902\n",
      "Compilation failed: Command '['/usr/local/cuda-12.1/bin/nvcc', '--generate-dependencies-with-compile', '--dependency-output', 'lstm_pointwise.cuda.o.d', '-ccbin', '/home/lucy/anaconda3/envs/deep/bin/x86_64-conda-linux-gnu-cc', '-DTORCH_EXTENSION_NAME=lstm_HS32', '-I', '/home/lucy/anaconda3/envs/deep/lib/python3.11/site-packages/torch/include', '-I', '/home/lucy/anaconda3/envs/deep/lib/python3.11/site-packages/torch/include/torch/csrc/api/include', '-I', '/home/lucy/anaconda3/envs/deep/lib/python3.11/site-packages/torch/include/TH', '-I', '/home/lucy/anaconda3/envs/deep/lib/python3.11/site-packages/torch/include/THC', '-I', '/usr/local/cuda-12.1/include', '-I', '/home/lucy/anaconda3/envs/deep/include/python3.11', '-D_GLIBCXX_USE_CXX11_ABI=0', '-D__CUDA_NO_HALF_OPERATORS__', '-D__CUDA_NO_HALF_CONVERSIONS__', '-D__CUDA_NO_BFLOAT16_CONVERSIONS__', '-D__CUDA_NO_HALF2_OPERATORS__', '--expt-relaxed-constexpr', '-gencode=arch=compute_52,code=compute_52', '-gencode=arch=compute_52,code=sm_52', '--compiler-options', '-fPIC', '-Xptxas=\"-v\"', '-gencode', 'arch=compute_80,code=compute_80', '-res-usage', '--use_fast_math', '-O3', '-Xptxas', '-O3', '--extra-device-vectorization', '-DSLSTM_HIDDEN_SIZE=32', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=2', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__', '-std=c++17', '-c', './xlstm/xlstm/blocks/slstm/src/cuda/lstm_pointwise.cu', '-o', 'lstm_pointwise.cuda.o']' returned non-zero exit status 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cc1plus: fatal error: ./xlstm/xlstm/blocks/slstm/src/cuda/lstm_pointwise.cu: No such file or directory\n",
      "compilation terminated.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['PATH'] = '/usr/local/cuda-12.1/bin:' + os.environ['PATH']\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-12.1/lib64:' + os.environ.get('LD_LIBRARY_PATH', '')\n",
    "\n",
    "# Clean previous builds\n",
    "extension_cache = \"/home/lucy/.cache/torch_extensions/\"\n",
    "if os.path.exists(extension_cache):\n",
    "    subprocess.run(['rm', '-rf', extension_cache])\n",
    "\n",
    "# Verify PyTorch configuration\n",
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "else:\n",
    "    print(\"CUDA not available\")\n",
    "\n",
    "# Rebuild the extension\n",
    "try:\n",
    "    subprocess.run([\n",
    "        '/usr/local/cuda-12.1/bin/nvcc',\n",
    "        '--generate-dependencies-with-compile',\n",
    "        '--dependency-output', 'lstm_pointwise.cuda.o.d',\n",
    "        '-ccbin', '/home/lucy/anaconda3/envs/deep/bin/x86_64-conda-linux-gnu-cc',\n",
    "        '-DTORCH_EXTENSION_NAME=lstm_HS32',\n",
    "        '-I', '/home/lucy/anaconda3/envs/deep/lib/python3.11/site-packages/torch/include',\n",
    "        '-I', '/home/lucy/anaconda3/envs/deep/lib/python3.11/site-packages/torch/include/torch/csrc/api/include',\n",
    "        '-I', '/home/lucy/anaconda3/envs/deep/lib/python3.11/site-packages/torch/include/TH',\n",
    "        '-I', '/home/lucy/anaconda3/envs/deep/lib/python3.11/site-packages/torch/include/THC',\n",
    "        '-I', '/usr/local/cuda-12.1/include',\n",
    "        '-I', '/home/lucy/anaconda3/envs/deep/include/python3.11',\n",
    "        '-D_GLIBCXX_USE_CXX11_ABI=0',\n",
    "        '-D__CUDA_NO_HALF_OPERATORS__',\n",
    "        '-D__CUDA_NO_HALF_CONVERSIONS__',\n",
    "        '-D__CUDA_NO_BFLOAT16_CONVERSIONS__',\n",
    "        '-D__CUDA_NO_HALF2_OPERATORS__',\n",
    "        '--expt-relaxed-constexpr',\n",
    "        '-gencode=arch=compute_52,code=compute_52',\n",
    "        '-gencode=arch=compute_52,code=sm_52',\n",
    "        '--compiler-options', '-fPIC',\n",
    "        '-Xptxas=\"-v\"',\n",
    "        '-gencode', 'arch=compute_80,code=compute_80',\n",
    "        '-res-usage',\n",
    "        '--use_fast_math',\n",
    "        '-O3',\n",
    "        '-Xptxas', '-O3',\n",
    "        '--extra-device-vectorization',\n",
    "        '-DSLSTM_HIDDEN_SIZE=32',\n",
    "        '-DSLSTM_BATCH_SIZE=8',\n",
    "        '-DSLSTM_NUM_HEADS=4',\n",
    "        '-DSLSTM_NUM_STATES=2',\n",
    "        '-DSLSTM_DTYPE_B=float',\n",
    "        '-DSLSTM_DTYPE_R=__nv_bfloat16',\n",
    "        '-DSLSTM_DTYPE_W=__nv_bfloat16',\n",
    "        '-DSLSTM_DTYPE_G=__nv_bfloat16',\n",
    "        '-DSLSTM_DTYPE_S=__nv_bfloat16',\n",
    "        '-DSLSTM_DTYPE_A=float',\n",
    "        '-DSLSTM_NUM_GATES=4',\n",
    "        '-DSLSTM_SIMPLE_AGG=true',\n",
    "        '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false',\n",
    "        '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0',\n",
    "        '-DSLSTM_FORWARD_CLIPVAL_VALID=false',\n",
    "        '-DSLSTM_FORWARD_CLIPVAL=0.0',\n",
    "        '-U__CUDA_NO_HALF_OPERATORS__',\n",
    "        '-U__CUDA_NO_HALF_CONVERSIONS__',\n",
    "        '-U__CUDA_NO_BFLOAT16_OPERATORS__',\n",
    "        '-U__CUDA_NO_BFLOAT16_CONVERSIONS__',\n",
    "        '-U__CUDA_NO_BFLOAT162_OPERATORS__',\n",
    "        '-U__CUDA_NO_BFLOAT162_CONVERSIONS__',\n",
    "        '-std=c++17',\n",
    "        '-c', './xlstm/xlstm/blocks/slstm/src/cuda/lstm_pointwise.cu',\n",
    "        '-o', 'lstm_pointwise.cuda.o'\n",
    "    ], check=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Compilation failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstm",
   "language": "python",
   "name": "xlstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
