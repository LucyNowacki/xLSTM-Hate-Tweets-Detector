{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys \n",
    "\n",
    "sys.path.append('..')\n",
    "from omegaconf import OmegaConf\n",
    "from pprint import pprint\n",
    "from dacite import from_dict\n",
    "from dacite import Config as DaciteConfig\n",
    "import torch\n",
    "\n",
    "from xlstm.xlstm_lm_model import xLSTMLMModel, xLSTMLMModelConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new model to load the checkpoint into\n",
    "xlstm_cfg = \"\"\" \n",
    "vocab_size: 50304\n",
    "context_length: 516      \n",
    "num_blocks: 4 #!\n",
    "embedding_dim: 768 #!\n",
    "tie_weights: false\n",
    "weight_decay_on_embedding: false\n",
    "mlstm_block:\n",
    "  mlstm:\n",
    "    conv1d_kernel_size: 2\n",
    "    qkv_proj_blocksize: 2\n",
    "    num_heads: 2\n",
    "\"\"\"\n",
    "cfg = OmegaConf.create(xlstm_cfg)\n",
    "cfg = from_dict(data_class=xLSTMLMModelConfig, data=OmegaConf.to_container(cfg), config=DaciteConfig(strict=True))\n",
    "model_new = xLSTMLMModel(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xLSTMLMModelConfig(mlstm_block=mLSTMBlockConfig(mlstm=mLSTMLayerConfig(proj_factor=2.0,\n",
      "                                                                       round_proj_up_dim_up=True,\n",
      "                                                                       round_proj_up_to_multiple_of=64,\n",
      "                                                                       _proj_up_dim=1536,\n",
      "                                                                       conv1d_kernel_size=2,\n",
      "                                                                       qkv_proj_blocksize=2,\n",
      "                                                                       num_heads=2,\n",
      "                                                                       embedding_dim=768,\n",
      "                                                                       bias=False,\n",
      "                                                                       dropout=0.0,\n",
      "                                                                       context_length=516,\n",
      "                                                                       _num_blocks=4,\n",
      "                                                                       _inner_embedding_dim=1536)),\n",
      "                   slstm_block=None,\n",
      "                   context_length=516,\n",
      "                   num_blocks=4,\n",
      "                   embedding_dim=768,\n",
      "                   add_post_blocks_norm=True,\n",
      "                   bias=False,\n",
      "                   dropout=0.0,\n",
      "                   slstm_at=[],\n",
      "                   _block_map='0,0,0,0',\n",
      "                   vocab_size=50304,\n",
      "                   tie_weights=False,\n",
      "                   weight_decay_on_embedding=False,\n",
      "                   add_embedding_dropout=False)\n"
     ]
    }
   ],
   "source": [
    "pprint(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = torch.randint(0, 50304, (1, 32)).to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = model_new.to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xLSTMLMModel(\n",
       "  (xlstm_block_stack): xLSTMBlockStack(\n",
       "    (blocks): ModuleList(\n",
       "      (0-3): 4 x mLSTMBlock(\n",
       "        (xlstm_norm): LayerNorm()\n",
       "        (xlstm): mLSTMLayer(\n",
       "          (proj_up): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (q_proj): LinearHeadwiseExpand(in_features=1536, num_heads=768, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "          (k_proj): LinearHeadwiseExpand(in_features=1536, num_heads=768, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "          (v_proj): LinearHeadwiseExpand(in_features=1536, num_heads=768, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "          (conv1d): CausalConv1d(\n",
       "            (conv): Conv1d(1536, 1536, kernel_size=(2,), stride=(1,), padding=(1,), groups=1536)\n",
       "          )\n",
       "          (conv_act_fn): SiLU()\n",
       "          (mlstm_cell): mLSTMCell(\n",
       "            (igate): Linear(in_features=4608, out_features=2, bias=True)\n",
       "            (fgate): Linear(in_features=4608, out_features=2, bias=True)\n",
       "            (outnorm): MultiHeadLayerNorm()\n",
       "          )\n",
       "          (ogate_act_fn): SiLU()\n",
       "          (proj_down): Linear(in_features=1536, out_features=768, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_blocks_norm): LayerNorm()\n",
       "  )\n",
       "  (token_embedding): Embedding(50304, 768)\n",
       "  (emb_dropout): Identity()\n",
       "  (lm_head): Linear(in_features=768, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = model_new(x_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 50304])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 2.95 GiB of which 7.25 MiB is free. Process 63280 has 141.38 MiB memory in use. Process 92007 has 83.38 MiB memory in use. Process 277123 has 91.38 MiB memory in use. Process 279082 has 995.38 MiB memory in use. Including non-PyTorch memory, this process has 1.65 GiB memory in use. Of the allocated memory 1.46 GiB is allocated by PyTorch, and 130.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x_in\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;241m1\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     y, state \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_new\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     y_new_step\u001b[38;5;241m.\u001b[39mappend(y)\n\u001b[1;32m      6\u001b[0m y_new_step \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(y_new_step, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/MachineLearning/xLSTM/xlstm/test/../xlstm/xlstm_lm_model.py:61\u001b[0m, in \u001b[0;36mxLSTMLMModel.step\u001b[0;34m(self, idx, state, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding(idx)\n\u001b[1;32m     60\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb_dropout(x)\n\u001b[0;32m---> 61\u001b[0m x, state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxlstm_block_stack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits, state\n",
      "File \u001b[0;32m~/Documents/MachineLearning/xLSTM/xlstm/test/../xlstm/xlstm_block_stack.py:132\u001b[0m, in \u001b[0;36mxLSTMBlockStack.step\u001b[0;34m(self, x, state)\u001b[0m\n\u001b[1;32m    129\u001b[0m     state \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block_idx, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[0;32m--> 132\u001b[0m     x, state[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblock_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblock_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mblock_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_blocks_norm(x)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, state\n",
      "File \u001b[0;32m~/Documents/MachineLearning/xLSTM/xlstm/test/../xlstm/blocks/xlstm_block.py:82\u001b[0m, in \u001b[0;36mxLSTMBlock.step\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m---> 82\u001b[0m     x_xlstm, xlstm_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxlstm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxlstm_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m x_xlstm\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/MachineLearning/xLSTM/xlstm/test/../xlstm/blocks/mlstm/layer.py:147\u001b[0m, in \u001b[0;36mmLSTMLayer.step\u001b[0;34m(self, x, mlstm_state, conv_state)\u001b[0m\n\u001b[1;32m    144\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(x_mlstm_conv_act)\n\u001b[1;32m    145\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(x_mlstm)\n\u001b[0;32m--> 147\u001b[0m h_tilde_state, mlstm_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlstm_cell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlstm_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlstm_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m h_tilde_state_skip \u001b[38;5;241m=\u001b[39m h_tilde_state \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearnable_skip \u001b[38;5;241m*\u001b[39m x_mlstm_conv_act)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# output / z branch\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MachineLearning/xLSTM/xlstm/test/../xlstm/blocks/mlstm/cell.py:117\u001b[0m, in \u001b[0;36mmLSTMCell.step\u001b[0;34m(self, q, k, v, mlstm_state, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m n_state\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (B, NH, DH, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected n_state shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(B,\u001b[38;5;250m \u001b[39mNH,\u001b[38;5;250m \u001b[39mDH,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_state\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m m_state\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (B, NH, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected m_state shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(B,\u001b[38;5;250m \u001b[39mNH,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm_state\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 117\u001b[0m h_state, mlstm_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend_fn_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mm_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43migate_preact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43migate_preact\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfgate_preact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfgate_preact\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, NH, 1 DH), ((B, NH, DH, DH), (B, NH, DH, 1), (B, NH, 1, 1))\u001b[39;00m\n\u001b[1;32m    128\u001b[0m h_state_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutnorm(h_state)  \u001b[38;5;66;03m# (B, NH, S, DH)\u001b[39;00m\n\u001b[1;32m    129\u001b[0m h_state_norm \u001b[38;5;241m=\u001b[39m h_state_norm\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, S, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, NH, S, DH) -> (B, S, NH, DH) -> (B, S, H)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MachineLearning/xLSTM/xlstm/test/../xlstm/blocks/mlstm/backends.py:135\u001b[0m, in \u001b[0;36mrecurrent_step_stabilized_simple\u001b[0;34m(c_state, n_state, m_state, q, k, v, igate_preact, fgate_preact, eps, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m ig_act \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(igate_preact \u001b[38;5;241m-\u001b[39m m_state_new)  \u001b[38;5;66;03m# (B, NH, 1, 1)\u001b[39;00m\n\u001b[1;32m    133\u001b[0m k_scaled \u001b[38;5;241m=\u001b[39m k \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(DH)\n\u001b[0;32m--> 135\u001b[0m c_state_new \u001b[38;5;241m=\u001b[39m \u001b[43mfg_act\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mig_act\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mk_scaled\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, NH, DH, DH)\u001b[39;00m\n\u001b[1;32m    136\u001b[0m n_state_new \u001b[38;5;241m=\u001b[39m fg_act \u001b[38;5;241m*\u001b[39m n_state \u001b[38;5;241m+\u001b[39m ig_act \u001b[38;5;241m*\u001b[39m k_scaled  \u001b[38;5;66;03m# (B, NH, DH, 1)\u001b[39;00m\n\u001b[1;32m    138\u001b[0m h_num \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m@\u001b[39m c_state_new  \u001b[38;5;66;03m# (B, NH, 1, DH)\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 2.95 GiB of which 7.25 MiB is free. Process 63280 has 141.38 MiB memory in use. Process 92007 has 83.38 MiB memory in use. Process 277123 has 91.38 MiB memory in use. Process 279082 has 995.38 MiB memory in use. Including non-PyTorch memory, this process has 1.65 GiB memory in use. Of the allocated memory 1.46 GiB is allocated by PyTorch, and 130.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "y_new_step = []\n",
    "state = None\n",
    "for x in x_in.split(1, dim=1):\n",
    "    y, state = model_new.step(x, state)\n",
    "    y_new_step.append(y)\n",
    "y_new_step = torch.cat(y_new_step, dim=1)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 50304])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new_step.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.9605e-08,  4.1723e-07,  5.9605e-08,  ...,  1.0431e-07,\n",
       "           1.1921e-07,  3.7253e-07],\n",
       "         [-2.3842e-07,  8.9407e-08,  5.9605e-08,  ..., -2.8312e-07,\n",
       "           1.7881e-07,  3.5763e-07],\n",
       "         [ 2.0862e-07, -2.9802e-08,  4.7684e-07,  ...,  3.5763e-07,\n",
       "           3.5763e-07,  5.9605e-08],\n",
       "         ...,\n",
       "         [ 5.9605e-08,  5.9605e-07, -5.9605e-08,  ..., -8.9407e-08,\n",
       "           5.9605e-08, -2.9802e-07],\n",
       "         [ 2.3842e-07, -1.9372e-07, -1.3411e-07,  ..., -3.2783e-07,\n",
       "           4.4703e-07, -5.9605e-08],\n",
       "         [-1.1921e-07, -2.3842e-07,  6.2585e-07,  ..., -5.0664e-07,\n",
       "          -1.7881e-07, -2.2352e-07]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new - y_new_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(y_new, y_new_step, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from dacite import from_dict\n",
    "from dacite import Config as DaciteConfig\n",
    "from xlstm import xLSTMLMModel, xLSTMLMModelConfig\n",
    "\n",
    "xlstm_cfg = \"\"\" \n",
    "vocab_size: 50304\n",
    "mlstm_block:\n",
    "  mlstm:\n",
    "    conv1d_kernel_size: 4\n",
    "    qkv_proj_blocksize: 4\n",
    "    num_heads: 4\n",
    "slstm_block:\n",
    "  slstm:\n",
    "    backend: vanilla\n",
    "    num_heads: 4\n",
    "    conv1d_kernel_size: 4\n",
    "    bias_init: powerlaw_blockdependent\n",
    "  feedforward:\n",
    "    proj_factor: 1.3\n",
    "    act_fn: gelu\n",
    "context_length: 256\n",
    "num_blocks: 7\n",
    "embedding_dim: 128\n",
    "slstm_at: [1]\n",
    "\"\"\"\n",
    "cfg = OmegaConf.create(xlstm_cfg)\n",
    "cfg = from_dict(data_class=xLSTMLMModelConfig, data=OmegaConf.to_container(cfg), config=DaciteConfig(strict=True))\n",
    "xlstm_stack = xLSTMLMModel(cfg)\n",
    "\n",
    "x = torch.randint(0, 50304, size=(4, 256)).to(\"cuda\")\n",
    "xlstm_stack = xlstm_stack.to(\"cuda\")\n",
    "y = xlstm_stack(x)\n",
    "y.shape[1:] == (256, 50304)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from xlstm import (\n",
    "    xLSTMBlockStack,\n",
    "    xLSTMBlockStackConfig,\n",
    "    mLSTMBlockConfig,\n",
    "    mLSTMLayerConfig,\n",
    "    sLSTMBlockConfig,\n",
    "    sLSTMLayerConfig,\n",
    "    FeedForwardConfig,\n",
    ")\n",
    "\n",
    "cfg = xLSTMBlockStackConfig(\n",
    "    mlstm_block=mLSTMBlockConfig(\n",
    "        mlstm=mLSTMLayerConfig(\n",
    "            conv1d_kernel_size=4, qkv_proj_blocksize=4, num_heads=4\n",
    "        )\n",
    "    ),\n",
    "    slstm_block=sLSTMBlockConfig(\n",
    "        slstm=sLSTMLayerConfig(\n",
    "            backend=\"vanilla\",\n",
    "            num_heads=4,\n",
    "            conv1d_kernel_size=4,\n",
    "            bias_init=\"powerlaw_blockdependent\",\n",
    "        ),\n",
    "        feedforward=FeedForwardConfig(proj_factor=1.3, act_fn=\"gelu\"),\n",
    "    ),\n",
    "    context_length=256,\n",
    "    num_blocks=7,\n",
    "    embedding_dim=128,\n",
    "    slstm_at=[1],\n",
    "\n",
    ")\n",
    "\n",
    "xlstm_stack = xLSTMBlockStack(cfg)\n",
    "\n",
    "x = torch.randn(4, 256, 128).to(\"cuda\")\n",
    "xlstm_stack = xlstm_stack.to(\"cuda\")\n",
    "y = xlstm_stack(x)\n",
    "y.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEEP",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
